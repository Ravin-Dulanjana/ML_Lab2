# -*- coding: utf-8 -*-
"""ml-layer9-190179m.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N2nAH5Nlk7C0AUor-LB_R2vHtn_EUrV0
"""

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
       print(os.path.join(dirname, filename))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error, r2_score
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import train_test_split, HalvingGridSearchCV
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier,  DecisionTreeRegressor
from xgboost import XGBRegressor

print(filenames)

"""**Read csv files**"""

trainData = pd.read_csv(os.path.join(dirname, filenames[1]))
validData = pd.read_csv(os.path.join(dirname, filenames[0]))
testData = pd.read_csv(os.path.join(dirname, filenames[2]))

trainData.head()

"""**Verify the presence of null values within the train data and drop them**"""

# Verify the presence of null values within the train data
trainNullCounts = trainData.isnull().sum()
print("Null counts in Train Data: \n {}".format(trainNullCounts))

# Remove rows with null values in the last four columns (target labels) of the train data
trainData = trainData.dropna(subset=trainData.columns[-4:], how='any')

# Replace null values with mean in datasets
trainData = trainData.fillna(trainData.mean())
validData = validData.fillna(validData.mean())
testData = testData.fillna(testData.mean())

trainData.head()

# Separate features and labels in datasets
trainFeatures = trainData.iloc[:, :-4]
trainLabels = trainData.iloc[:, -4:]
validFeatures = validData.iloc[:, :-4]
validLabels = validData.iloc[:, -4:]
testFeatures = testData.iloc[:, 1:]
ID = testData.iloc[:, 0]

# Extract the first label of the datasets
trainLabel1 = trainLabels.iloc[:,0]
validLabel1 = validLabels.iloc[:,0]
testLabel1 = testLabels.iloc[:,0]

# Extract the second label of the datasets
trainLabel2 = trainLabels.iloc[:,1].astype('int64')
validLabel2 = validLabels.iloc[:,1].astype('int64')

# Extract the third label of the datasets
trainLabel3 = trainLabels.iloc[:,2]
validLabel3 = validLabels.iloc[:,2]

# Extract the fourth label of the datasets
trainLabel4 = trainLabels.iloc[:,3]
validLabel4 = validLabels.iloc[:,3]

"""# Feature Engineering

**Visualize data in Labels**
"""

# Plotting the distribution of trainLabel1
labels, counts = np.unique(trainLabel1, return_counts=True)

plt.figure(figsize=(22, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.xlabel('Target Label 1')
plt.ylabel('Frequency')
plt.title('Frequency of Target Label 1')
plt.show()

# Plotting the distribution of trainLabel2
labels, counts = np.unique(trainLabel2, return_counts=True)

plt.figure(figsize=(22, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.xlabel('Target Label 2')
plt.ylabel('Frequency')
plt.title('Frequency of Target Label 2')
plt.show()

# Plotting the distribution of trainLabel3
labels, counts = np.unique(trainLabel3, return_counts=True)

plt.figure(figsize=(22, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.xlabel('Target Label 3')
plt.ylabel('Frequency')
plt.title('Frequency of Target Label 3')
plt.show()

# Plotting the distribution of trainLabel4
labels, counts = np.unique(trainLabel4, return_counts=True)

plt.figure(figsize=(22, 6))
plt.xticks(labels)
plt.bar(labels, counts)
plt.xlabel('Target Label 4')
plt.ylabel('Frequency')
plt.title('Frequency of Target Label 4')
plt.show()

#Compute the correlation matrix and create a heatmap
correlationMatrix = trainFeatures.corr()
mask = np.triu(np.ones_like(correlationMatrix))
plt.figure(figsize=(12, 12))
sns.heatmap(correlationMatrix, cmap='coolwarm', center=0, mask=mask)
plt.title("Correlation Matrix")
plt.show()

# Determine highly correlated features within the training dataset
correlationThreshold = 0.85
highlyCorrelated = set()
for i in range(len(correlationMatrix.columns)):
    for j in range(i):
        if abs(correlationMatrix.iloc[i, j]) > correlationThreshold:
            colname = correlationMatrix.columns[i]
            highlyCorrelated.add(colname)
print(highlyCorrelated)

# Exclude features that were previously identified as having high correlation from all datasets
trainFeatures = trainFeatures.drop(columns=highlyCorrelated)
validFeatures = validFeatures.drop(columns=highlyCorrelated)
testFeatures = testFeatures.drop(columns=highlyCorrelated)

# Display the filtered feature counts
print("Train features after filtering: {}".format(trainFeatures.shape))
print("Valid features after filtering: {}".format(validFeatures.shape))
print("Test features after filtering: {}".format(testFeatures.shape))

# Detect the attributes strongly linked to the target variable by analyzing the training dataset
correlationWithTarget = trainFeatures.corrwith(trainLabel1)
correlationThreshold = 0.05
highlyCorrelatedFeatures = correlationWithTarget[correlationWithTarget.abs() > correlationThreshold]
print(highlyCorrelatedFeatures)

# Drop the features with low correlation in the datasets
trainFeatures = trainFeatures[highlyCorrelatedFeatures.index]
validFeatures = validFeatures[highlyCorrelatedFeatures.index]
testFeatures = testFeatures[highlyCorrelatedFeatures.index]

# Display the filtered feature counts  of the datasets
print("Train features after filtering: {}".format(trainFeatures.shape))
print("Valid features after filtering: {}".format(validFeatures.shape))
print("Test features after filtering: {}".format(testFeatures.shape))

# Standardize the features
scaler = StandardScaler()
standardizedTrainFeatures = scaler.fit_transform(trainFeatures)
standardizedValidFeatures = scaler.transform(validFeatures)
standardizedTestFeatures = scaler.transform(testFeatures)

varianceThreshold = 0.99

# Apply PCA with the determined no. of components
pca = PCA(n_components=varianceThreshold, svd_solver='full')
pcaTrainResult = pca.fit_transform(standardizedTrainFeatures)
pcaValidResult = pca.transform(standardizedValidFeatures)
pcaTestResult = pca.transform(standardizedTestFeatures)

# Explained variance ratio after dimensionality reduction
explainedVarianceRatioReduced = pca.explained_variance_ratio_
print("Explained Variance Ratio following Dimensionality Reduction:", explainedVarianceRatioReduced)

# Visualize Variance Explained Ratio
plt.figure(figsize=(18, 10))
plt.bar(range(1, pcaTrainResult.shape[1] + 1), explainedVarianceRatioReduced)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio for Each Principal Component (Reduced Dimensionality)')
plt.show()

# Display the reduced feature matrices
print("Shape of the Reduced Training Feature Matrix: {}".format(pcaTrainResult.shape))
print("Shape of the Reduced Validation Feature Matrix: {}".format(pcaValidResult.shape))
print("Shape of the Reduced Testing Feature Matrix: {}".format(pcaTestResult.shape))

svm = SVC()
paramGrid = {
    'C': [100],
    'kernel': ['rbf'],
    'gamma': [0.001]
}

hgridSearch = HalvingGridSearchCV(svm, paramGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
hgridSearch.fit(pcaTrainResult, trainLabel1)

bestParams = hgridSearch.best_params_
bestModel = hgridSearch.best_estimator_

# Evaluate the best model on the test set
validAccuracy = bestModel.score(pcaValidResult, validLabel1)
print("Validation Accuracy: {}".format(validAccuracy))

predictionTrain = bestModel.predict(pcaTrainResult)
accuracy = accuracy_score(trainLabel1, predictionTrain)
precision = precision_score(trainLabel1, predictionTrain, average='weighted' , zero_division=1)
recall = recall_score(trainLabel1, predictionTrain, average='weighted')

print(f"Metrics for SVM on train data:")
print(f"  - Accuracy: {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall: {recall:.4f}\n")

predictionValid = bestModel.predict(pcaValidResult)
accuracy = accuracy_score(validLabel1, predictionValid)
precision = precision_score(validLabel1, predictionValid, average='weighted', zero_division=1)
recall = recall_score(validLabel1, predictionValid, average='weighted')

print(f"Metrics for SVM on validation data:")
print(f"  - Accuracy: {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall: {recall:.4f}\n")

predictionTest1 = bestModel.predict(pcaTestResult)

svm = SVC()
paramGrid = {
    'C': [100],
    'kernel': ['rbf'],
    'gamma': [0.001]
}

hgridSearch = HalvingGridSearchCV(svm, paramGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
hgridSearch.fit(pcaTrainResult, trainLabel2)

bestParams = hgridSearch.best_params_
bestModel = hgridSearch.best_estimator_

# Evaluate the best model on the test set
validAccuracy = bestModel.score(pcaValidResult, validLabel1)
print("Validation Accuracy: {}".format(validAccuracy))

predictionTrain = bestModel.predict(pcaTrainResult)
accuracy = accuracy_score(trainLabel2, predictionTrain)
precision = precision_score(trainLabel2, predictionTrain, average='weighted' , zero_division=1)
recall = recall_score(trainLabel2, predictionTrain, average='weighted')

print(f"Metrics for SVM on train data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionValid = bestModel.predict(pcaValidResult)
accuracy = accuracy_score(validLabel2, predictionValid)
precision = precision_score(validLabel2, predictionValid, average='weighted', zero_division=1)
recall = recall_score(validLabel2, predictionValid, average='weighted')

print(f"Metrics for SVM on validation data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionTest2 = bestModel.predict(pcaTestResult)

svm = SVC()
paramGrid = {
    'C': [100],
    'kernel': ['rbf'],
    'gamma': [0.001]
}

hgridSearch = HalvingGridSearchCV(svm, paramGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
hgridSearch.fit(pcaTrainResult, trainLabel3)

bestParams = hgridSearch.best_params_
bestModel = hgridSearch.best_estimator_

# Evaluate the best model on the test set
validAccuracy = bestModel.score(pcaValidResult, validLabel1)
print("Validation Accuracy: {}".format(validAccuracy))

predictionTrain = bestModel.predict(pcaTrainResult)
accuracy = accuracy_score(trainLabel3, predictionTrain)
precision = precision_score(trainLabel3, predictionTrain, average='weighted' , zero_division=1)
recall = recall_score(trainLabel3, predictionTrain, average='weighted')

print(f"Metrics for SVM on train data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionValid = bestModel.predict(pcaValidResult)
accuracy = accuracy_score(validLabel3, predictionValid)
precision = precision_score(validLabel3, predictionValid, average='weighted', zero_division=1)
recall = recall_score(validLabel3, predictionValid, average='weighted')

print(f"Metrics for SVM on validation data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionTest3 = bestModel.predict(pcaTestResult)

svm = SVC()
paramGrid = {
    'C': [100],
    'kernel': ['rbf'],
    'gamma': [0.001]
}

hgridSearch = HalvingGridSearchCV(svm, paramGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
hgridSearch.fit(pcaTrainResult, trainLabel4)

bestParams = hgridSearch.best_params_
bestModel = hgridSearch.best_estimator_

# Evaluate the best model on the test set
validAccuracy = bestModel.score(pcaValidResult, validLabel1)
print("Validation Accuracy: {}".format(validAccuracy))

predictionTrain = bestModel.predict(pcaTrainResult)
accuracy = accuracy_score(trainLabel4, predictionTrain)
precision = precision_score(trainLabel4, predictionTrain, average='weighted' , zero_division=1)
recall = recall_score(trainLabel4, predictionTrain, average='weighted')

print(f"Metrics for SVM on train data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionValid = bestModel.predict(pcaValidResult)
accuracy = accuracy_score(validLabel4, predictionValid)
precision = precision_score(validLabel4, predictionValid, average='weighted', zero_division=1)
recall = recall_score(validLabel4, predictionValid, average='weighted')

print(f"Metrics for SVM on validation data:")
print(f"  - Accuracy: {accuracy:.5f}")
print(f"  - Precision: {precision:.5f}")
print(f"  - Recall: {recall:.5f}\n")

predictionTest4 = bestModel.predict(pcaTestResult)

# define method to create the dataframe and save it as a csv file
def create_csv(ID, predLabel1, predLabel2, predLabel3, predLabel4, destination):
  df = pd.DataFrame()

  df.insert(loc=0, column='ID', value=ID)
  df.insert(loc=1, column='label_1', value=predLabel1)
  df.insert(loc=2, column='label_2', value=predLabel2)
  df.insert(loc=3, column='label_3', value=predLabel3)
  df.insert(loc=4, column='label_4', value=predLabel4)
  df.to_csv(destination, index=False)


# create the csv output file
create_csv(ID, predictionTest1, predictionTest2, predictionTest3, predictionTest4,'190179M_Layer9.csv')